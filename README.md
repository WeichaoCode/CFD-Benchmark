# Code Generation Benchmark Framework
This repository provides a benchmark suite to evaluate the performance of large language models (LLMs) on
**code generation tasks for solving partial differential equations (PDEs)**. It includes prompt templates, 
auto-evaluation utilities, and solution validation for physics-based problems.

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ __pycache__
â”‚   â”œâ”€â”€ utils.cpython-310.pyc
â”‚   â””â”€â”€ utils.cpython-38.pyc
â”œâ”€â”€ compare     # compare the MSE
â”œâ”€â”€ compare_images # compare the fluid flow images
â”‚   â”œâ”€â”€ ground_truth   
â”‚   â”œâ”€â”€ prediction
â”‚   â””â”€â”€ table
â”œâ”€â”€ image  # save the fluid flow images
â”‚   â”œâ”€â”€ gemini
â”‚   â”œâ”€â”€ gpt-4o
â”‚   â”œâ”€â”€ haiku
â”‚   â”œâ”€â”€ o3-mini
â”‚   â””â”€â”€ sonnet-35
â”œâ”€â”€ prompt
â”‚   â”œâ”€â”€ PDE_TASK_QUESTION_ONLY.json
â”‚   â””â”€â”€ prompts.json
â”œâ”€â”€ report # log files and report
â”œâ”€â”€ results # .npy files
â”‚   â”œâ”€â”€ prediction
â”‚   â””â”€â”€ solution # groud truth 
â”œâ”€â”€ solution # ground truth python code
â”œâ”€â”€ solver # generate python code 
â”‚   â”œâ”€â”€ gemini
â”‚   â”œâ”€â”€ gpt-4o
â”‚   â”œâ”€â”€ haiku
â”‚   â”œâ”€â”€ o3-mini
â”‚   â””â”€â”€ sonnet-35
â””â”€â”€ table # create table contains MSE ...
```
---

## ğŸš€ Quickstart

### 1. ğŸ› ï¸ Installation

We recommend using **conda** to manage dependencies:

```bash
conda create -n pde_benchmark python=3.10
conda activate pde_benchmark

# Install Python dependencies
pip install -r requirements.txt
```
Make sure to install extra dependencies manually if needed:
```bash
pip install openai boto3 matplotlib scikit-learn opencv-python scikit-image
```

### 2. ğŸ”‘ API Keys
Before running, make sure your ```OpenAI```, ```Gemini```, or ```Bedrock``` API credentials are set.

You can either export them in environment variables:
```bash
export OPENAI_API_KEY=your-key
export GEMINI_API_KEY=your-key
```
Or directly edit the ```utils.py``` to insert your keys.

## ğŸ’» Run the Benchmark
To run the benchmark pipeline for all prompts and models:
```bash
python train_plain_prompts.py
```
This script performs the following steps:

* **Load Prompts**  
  Loads each task prompt from the `prompt/` directory.

* **Call LLM to Generate Code**  
  Uses the selected Large Language Model (LLM) (e.g., GPT-4o, Gemini, Claude, etc.) to generate solver code for the given PDE problem.

* **Save Generated Code**  
  Saves the generated Python solver scripts into the `solver/` directory.

* **Execute and Evaluate**  
  Runs each solver, compares the results with the ground-truth reference in the `solution/` directory.

* **Log Results**  
  * Performance metrics and runtime are logged into the `report/` folder.  
  * Visual outputs are saved in the `image/` folder.  
  * Final comparison results are stored in the `results/` directory.
  * Final comparison results tables are stored in the `table/` directory.

## ğŸ“Š Output and Evaluation
* Execution results (error logs, pass/fail): saved in `report/`

* Generated solvers: `solver/`

* Reference solutions: `solution/`

* Performance images: `image/`

* Comparison tools: `compare/`, `compare_images/`

* Summary tables: `table/`

## ğŸ§ª Add Your Own Task
* Add a new `prompt_name.json` under `prompt/`

* Add the corresponding ground truth solution under `solution/`
