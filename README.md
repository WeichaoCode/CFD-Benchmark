# Code Generation Benchmark Framework
This repository provides a benchmark suite to evaluate the performance of large language models (LLMs) on
**code generation tasks for solving partial differential equations (PDEs)**. It includes prompt templates, 
auto-evaluation utilities, and solution validation for physics-based problems.

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ __pycache__
â”‚   â”œâ”€â”€ utils.cpython-310.pyc
â”‚   â””â”€â”€ utils.cpython-38.pyc
â”œâ”€â”€ clean.py
â”œâ”€â”€ folder_structure.md
â”œâ”€â”€ prompt
â”‚   â”œâ”€â”€ PDE_TASK_QUESTION_ONLY.json
â”‚   â””â”€â”€ prompts.json
â”œâ”€â”€ results
â”‚   â””â”€â”€ solution
â”œâ”€â”€ solution
â”‚   â”œâ”€â”€ 1D_Burgers_Equation.py
â”‚   â”œâ”€â”€ 1D_Diffusion.py
â”‚   â”œâ”€â”€ 1D_Euler_Shock_Tube.py
â”‚   â”œâ”€â”€ 1D_KdV_Burgers_Equation.py
â”‚   â”œâ”€â”€ 1D_Linear_Convection_adams.py
â”‚   â”œâ”€â”€ 1D_Linear_Convection_explicit_euler.py
â”‚   â”œâ”€â”€ 1D_Linear_Convection_pred_corr.py
â”‚   â”œâ”€â”€ 1D_Linear_Convection_rk.py
â”‚   â”œâ”€â”€ 1D_Nonlinear_Convection_LW.py
â”‚   â”œâ”€â”€ 1D_Nonlinear_Convection_Lax.py
â”‚   â”œâ”€â”€ 1D_Nonlinear_Convection_Mk.py
â”‚   â”œâ”€â”€ 2D_Burgers_Equation.py
â”‚   â”œâ”€â”€ 2D_Convection.py
â”‚   â”œâ”€â”€ 2D_Diffusion.py
â”‚   â”œâ”€â”€ 2D_Diffusion_FVM.py
â”‚   â”œâ”€â”€ 2D_Inviscid_Burgers_FOU.py
â”‚   â”œâ”€â”€ 2D_Inviscid_Burgers_MK.py
â”‚   â”œâ”€â”€ 2D_Laplace_Equation.py
â”‚   â”œâ”€â”€ 2D_Linear_Convection.py
â”‚   â”œâ”€â”€ 2D_Navier_Stokes_Cavity.py
â”‚   â”œâ”€â”€ 2D_Navier_Stokes_Channel.py
â”‚   â”œâ”€â”€ 2D_Possion.py
â”‚   â”œâ”€â”€ 2D_Rayleigh_Benard_Convection.py
â”‚   â”œâ”€â”€ 2D_Shear_Flow_With_Tracer.py
â”‚   â”œâ”€â”€ 2D_Steady_Heat_Equation_Gauss.py
â”‚   â”œâ”€â”€ 2D_Steady_Heat_Equation_Jac.py
â”‚   â”œâ”€â”€ 2D_Steady_Heat_Equation_SOR.py
â”‚   â”œâ”€â”€ 2D_Unsteady_Heat_Equation_ADI.py
â”‚   â”œâ”€â”€ 2D_Unsteady_Heat_Equation_DF.py
â”‚   â”œâ”€â”€ 2D_Unsteady_Heat_Equation_SE.py
â”‚   â”œâ”€â”€ Flow_Past_Circular_Cylinder.py
â”‚   â”œâ”€â”€ Fully_Developed_Turbulent_Channel_Flow.py
â”‚   â”œâ”€â”€ Lane_Emden_Equation.py
â”‚   â”œâ”€â”€ Lid_Driven_Cavity.py
â”‚   â”œâ”€â”€ Pipe_Flow_Disk_EVP.py
â”‚   â””â”€â”€ Vortex_Roll_Up.py
â”œâ”€â”€ train_plain_prompts.py
â”œâ”€â”€ tree.py
â””â”€â”€ utils.py
```
## ğŸ“ Repository Structure Explained
This repository is designed to **benchmark LLM-generated PDE solvers** against ground-truth implementations. 
Here's an overview of each folder and file:
### ğŸ“‚ `prompt/`
* `PDE_TASK_QUESTION_ONLY.json`: Main prompt configuration for generation
### ğŸ“‚ `solution/`
* Holds ground-truth reference implementations of various PDE solvers (e.g., finite difference/volume/time methods).
* Each file solves a specific PDE (e.g., `1D_Burgers_Equation.py`, `2D_Laplace_Equation.py`, etc.).
* Used for comparison against LLM-generated solutions.
### ğŸ“‚ `results/solution/`
* Stores outputs from the ground truth for comparison (e.g., `.npy` files).
### ğŸ“„ `train_plain_prompts.py`
* Main entry point for running the benchmark.
* This script:
  * Generate prompts for LLM using `PDE_TASK_QUESTION_ONLY.json`
  * Calls the selected LLM to generate solver code
  * Executes generated code and save the `.npy`
  * Compares results with reference solutions
  * Post-processing, create and save images / tables / log files
---
## ğŸš€ Quickstart

### 1. ğŸ› ï¸ Installation

We recommend using **conda** to manage dependencies:

```bash
conda create -n pde_benchmark python=3.10
conda activate pde_benchmark

# Install Python dependencies
pip install -r requirements.txt
```
Make sure to install extra dependencies manually if needed:
```bash
pip install openai boto3 matplotlib scikit-learn opencv-python scikit-image
```

### 2. ğŸ”‘ API Keys
Before running, make sure your ```OpenAI```, ```Gemini```, or ```Bedrock``` API credentials are set.

You can either export them in environment variables:
```bash
export OPENAI_API_KEY=your-key
export GOOGLE_API_KEY=your-key
```
Or directly edit the ```utils.py``` to insert your keys.

---
## ğŸ’» Run the Benchmark
To run the benchmark pipeline for all prompts and models:
```bash
python train_plain_prompts.py
```
This script performs the following steps:

* **Load Prompts**  
  Loads each task prompt from the `prompt/` directory.

* **Call LLM to Generate Code**  
  Uses the selected Large Language Model (LLM) (e.g., GPT-4o, Gemini, Claude, etc.) to generate solver code for the given PDE problem.

* **Save Generated Code**  
  Saves the generated Python solver scripts into the `solver/` directory.

* **Execute and Evaluate**  
  Runs each solver, compares the results with the ground-truth reference in the `solution/` directory.

* **Log Results**  
  * Performance metrics and runtime are logged into the `report/` folder.  
  * Visual outputs are saved in the `image/` folder.  
  * Final comparison results are stored in the `results/` directory.
  * Final comparison results tables are stored in the `table/` directory.
---
## ğŸ“Š Output and Evaluation
* Execution results (error logs, pass/fail): saved in `report/`

* Generated solvers: `solver/`

* Reference solutions: `solution/`

* Performance images: `image/`

* Comparison tools: `compare/`, `compare_images/`

* Summary tables: `table/`
---

## ğŸ§¹ clean.py â€” Reset Environment Script
This script is designed to reset the working environment before running a new experiment.
It acts like a "make clean" command in traditional build systems.
```bash
python clean.py
```
---
## ğŸ§ª Add Your Own Task
* Add a new `prompt_name.json` under `prompt/`

* Add the corresponding ground truth solution under `solution/`
